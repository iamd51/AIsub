#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPU åŠŸèƒ½æ¸¬è©¦è…³æœ¬
ç”¨æ–¼é©—è­‰ Whisper GPU åŠ é€Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import warnings
warnings.filterwarnings("ignore", message=".*Failed to launch Triton kernels.*")
warnings.filterwarnings("ignore", message=".*falling back to a slower.*")

def test_gpu_support():
    """æ¸¬è©¦ GPU æ”¯æ´"""
    print("ğŸ” GPU æ”¯æ´æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦ PyTorch
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… GPU æ•¸é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£")
        return False
    
    # æ¸¬è©¦ Whisper
    try:
        import whisper
        print(f"âœ… Whisper ç‰ˆæœ¬: {whisper.__version__}")
        print(f"âœ… å¯ç”¨æ¨¡å‹: {', '.join(whisper.available_models())}")
    except ImportError:
        print("âŒ Whisper æœªå®‰è£")
        return False
    
    # æ¸¬è©¦æ¨¡å‹è¼‰å…¥
    try:
        print("\nğŸ§ª æ¸¬è©¦æ¨¡å‹è¼‰å…¥...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {device}")
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            model = whisper.load_model("tiny", device=device)
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        print(f"ğŸ“Š æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return False
    
    print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼GPU åŠ é€Ÿå·²æº–å‚™å°±ç·’")
    return True

def test_simple_transcription():
    """æ¸¬è©¦ç°¡å–®çš„è½‰éŒ„åŠŸèƒ½"""
    print("\nğŸ¤ ç°¡å–®è½‰éŒ„æ¸¬è©¦")
    print("=" * 50)
    
    try:
        import whisper
        import torch
        import numpy as np
        
        # å»ºç«‹æ¸¬è©¦éŸ³è¨Š (1ç§’çš„éœéŸ³)
        sample_rate = 16000
        duration = 1.0
        audio = np.zeros(int(sample_rate * duration), dtype=np.float32)
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {device}")
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            model = whisper.load_model("tiny", device=device)
            result = model.transcribe(audio)
        
        print("âœ… è½‰éŒ„æ¸¬è©¦å®Œæˆ")
        print(f"ğŸ“ çµæœ: '{result['text'].strip()}'")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ è½‰éŒ„æ¸¬è©¦å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Whisper GPU åŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)
    
    # åŸºæœ¬åŠŸèƒ½æ¸¬è©¦
    if not test_gpu_support():
        print("\nâŒ åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—")
        input("æŒ‰ Enter éµçµæŸ...")
        exit(1)
    
    # è½‰éŒ„åŠŸèƒ½æ¸¬è©¦
    if not test_simple_transcription():
        print("\nâš ï¸ è½‰éŒ„åŠŸèƒ½æ¸¬è©¦å¤±æ•—ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    
    print("\n" + "=" * 60)
    print("âœ… æ¸¬è©¦å®Œæˆï¼ä½ çš„ç³»çµ±å·²æº–å‚™å¥½ä½¿ç”¨ Whisper GPU åŠ é€Ÿ")
    print("ğŸ’¡ ç¾åœ¨å¯ä»¥å•Ÿå‹• whisper_subtitle_gui.py é–‹å§‹ä½¿ç”¨")
    
    input("\næŒ‰ Enter éµçµæŸ...")